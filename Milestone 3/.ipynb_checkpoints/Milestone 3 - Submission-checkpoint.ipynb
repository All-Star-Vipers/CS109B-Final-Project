{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the features described in the previous milestone, we attemped to create features from the text fields (`overview`,`tagline`,`plot`,`plot outline`, and `mpaa rating`) by clustering them based on key words.\n",
    "\n",
    "To do this, we first transformed each text field into a tf-idf matrix which gets word counts for each record and then scales down the word counts based on the occurence of the word in the entire training set. As part of the tf-idf transformation we:\n",
    "* \"stemmed\" the words (\"fishing\", \"fished\", \"fisher\" would all be reduced to \"fish\")\n",
    "* filtered out common english terms (\"a\", \"the\", etc.)\n",
    "* used n-grams of lengths 1,2,and 3 (\"John\",\"John likes\", \"John likes movies\", etc.)\n",
    "* limited the max dataset frequency of a term to 10% (essentially the size of a genre as there are 11 genres) and the minimum datasest frequency to 5 (based on previous models we'd seen)\n",
    "\n",
    "With the transformation complete we applied k-means clustering to the tf-idf matrix, tuning the number of clusters based on whether the cluster key-words made intuitive sense with the genres in that cluster. For instance, `overview_cluster_3` had key words of \"murderer\", \"town\", \"killer\", \"investigator\", and \"detective\", and the top genres in this cluster were Thriller, Horror, and Drama-Thriller. For `overview_cluster_4`, which had key words of \"Love\", \"falls\", \"woman\",\"man\", and \"story\", the top genres were \"Drama-Romance\", \"Drama\", and \"Comedy-Romance\".\n",
    "\n",
    "All the code and clusters can be viewed here: https://github.com/All-Star-Vipers/CS109B-Final-Project/blob/master/Milestone%203/Text%20Analysis.ipynb\n",
    "\n",
    "Credit for some of the code and general approach goes to: http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a Random Forest Classifier on our data, which essentially grows decision trees while randomizing the subset of predictors used to grow those trees. The model than takes the mode of the outcomes of all the trees and records that class as the model's prediction.\n",
    "\n",
    "With the sklearn package, we tuned a random forest classifier using grid search with 5-fold cross-validation on the following parameters:\n",
    "* `n_estimators`: the number of trees grown in the model. We did not vary this and set it at 500 under the assumption that more trees would improve model accuracy (with the trade-off being the amount of time taken to run the model)\n",
    "* `max_features`: the number of features in the subset used to train the tree. The more features selected, the more likely the model is to overfit. We used the 3 options provided by the sklearn package - auto, sqrt, and log2. \n",
    "* `min_samples_leaf`: the numeber of samples in a particular branch at which the tree stops growing. The lower the number, the more likely the tree is to overfit. We used 1, 10, and 100.\n",
    "* `class_weight`: the sklearn package offers built-in options to deal with class imbalance which is an issue for us, so we tuned the model on 3 parameters - None, balanced, and balanced_subsample.\n",
    "* `max_depth`: the depth to which individual decision trees are grown, with higher values increasing the chance of overfitting. We used 10, 100, and None for this parameter.\n",
    "\n",
    "We chose the f1 score as the metric to maximize for our grid search, which resulted in the following model being chosen:\n",
    "* `max_features` =\n",
    "* `min_samples_leaf`=\n",
    "* `class_weight` =\n",
    "* `max_depth` = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated model performance on our testing set (30% of the original dataset or ~3,000 records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed significantly better than a random classifier, with performance metrics on the test set as follows:\n",
    "* `accuracy` = \n",
    "* `precision` = \n",
    "* `recall` = \n",
    "* `f1` =\n",
    "\n",
    "We visualized the model's performance using both confusion matrices and ROC curves. The confusion matrix for the test set showed that the model performed well on genres like ____________, but more poorly on genres like ________. More problematic was that the model's performance on the training set yielded 100% accuracy, indicating potential overfitting during training. However, when we ran the model using smaller amounts of predictors (filtering based on feature importance), all of the performance metrics mentioned above declined as well. This bears further investigation.\n",
    "\n",
    "To see the model and notebook visualizations, please see: https://github.com/All-Star-Vipers/CS109B-Final-Project/blob/master/Milestone%203/Random%20Forest%20Model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of buzz that XGBoost is often a very strong classifier, and better than the Random Forest model. XGBoost is a GBM (generalized boosted model), and like RF, it uses decision trees. However, instead of completely randomizing the predictors in those trees, it learns from each previous tree. However, it is a model that include regularization.\n",
    "\n",
    "Using the xgboost and sklearn packages, we tuned a XGBoost classifier using grid search with 5-fold cross-validation on the following parameters, as our research indicated these to be most crucial:\n",
    "\n",
    "* gamma : Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "* min_child_weight : Minimum sum of instance weight(hessian) needed in a child.\n",
    "* max_depth : Maximum tree depth for base learners.\n",
    "* subsample : Subsample ratio of the training instance.\n",
    "* colsample_bytree : Subsample ratio of columns when constructing each tree.\n",
    "* reg_alpha : L1 regularization term on weights\n",
    "\n",
    "We also selected a relatively slow learning rate and large number of trees to be used. \n",
    "\n",
    "Again, we chose the f1 score as the metric to maximize for our grid search, which resulted in the following model being chosen:\n",
    "* `gamma` = 0.2\n",
    "* `min_child_weight`= 0\n",
    "* `max_depth` = 5\n",
    "* `subsample` = 0.6\n",
    "* `colsample_bytree` = 0.9\n",
    "* `reg_alpha` = 0.1\n",
    "* `eta` = 0.1\n",
    "* `n_estimators` = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While random forest grows trees independently of other trees, xgboost looks to minimize the residuals produced by the model by growing additional trees based on the residuals of the previous trees. This can lead xgboost to perform better than random forest if tuned correctly but also makes it more prone to overfit if the model fits the residuals too well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways we could improve these models:\n",
    "* `Tune on More Parameters`: We could look tune more parameters on the models, both by including completely new parameters (varying the `criterion` in the random forest model) or increasing the number of values for existing parameters (testing 500, 1000, and 2000 `max_features` for random forest)\n",
    "* `Include More Features`: We did attempt some PCA on a subset of the movie posters but did not do it for the entire dataset. IMDb also offers more features for the movies so we could include those. We could also try more clustering techniques, including agglomerative ones, on the text variables and see if the results improve performance.\n",
    "* `Change Genre Configuration`: Since we combined various genre classes together based on personal judgement, we might have combined some incorrectly making differences between the genres harder to distinguish than they are in reality.\n",
    "* `Consider Outputting Probability Distributions`: Due to the overlap between our genre categories (as some are defined as intersections of others), it might be interesting to see what the model's probability distribution by category looks like. For example, is it confident that movies whose actual label contains 'Comedy' fall under some category containing 'Comedy'? Or is it also predicting that a movie whose actual label contains 'Comedy' is likely to fall into 'Documentary' too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost model had the following results:\n",
    "* `Training Accuracy` = 100%. Across 11 classes, this was somewhat concerning.\n",
    "* `Test Accuracy` = 41%\n",
    "* `Precision Score` = 40%\n",
    "* `Recall Score` = 41%\n",
    "\n",
    "The most impactful features were popularity, overview_length, plot_length, plot_outline_length, and cast. I am a bit surprised that overview_length, plot_length, plot_outline_length should have been so impactful.\n",
    "\n",
    "Additionally, it's important to consider that our 41% accuracy was achieved across 11 classes, and that these classes have overlap. For example, it's not surprising that it's hard to distinguish between Comedy and RomCom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
