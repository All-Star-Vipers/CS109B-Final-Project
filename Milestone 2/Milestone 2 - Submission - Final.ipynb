{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS109b Final Project - Group 24  \n",
    "### Milestone 2\n",
    "\n",
    "Tyler Finkelstein  \n",
    "James Fallon  \n",
    "Aaron Myran  \n",
    "Ihsan Patel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was initially obtained using the TMDb popular movies API, which returned the ~20,000 most popular movies in TMDb's database along with some metadata for the movies. This data did not, however, have an IMDb ID for the movies, so we looped through each TMDb record and used its TMDb id to obtain the IMDb id and 15 other features (described in the feature selection section) through TMDb's search function. Movie posters were also pulled using the link provided by TMDb.\n",
    "\n",
    "Before pulling the IMDb data, we filtered the dataset for english language movies only to ensure the metadata (particularly things like plot summaries) remained consistent and based on the assumption that cultural differences could lead the same genre to have very different characteristics depending on the culture.\n",
    "\n",
    "We then pulled IMDb data using the `IMDBPY` library which yielded an additional 91 features for each movie not all of which were used (the ones that were used are described in the feature selection section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What We're Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the issues we’ve encountered with the datasets is the fact that the IMDb and TMDb genres do not always match up. We discussed a variety of options for dealing with this including. First, though, we agreed that there are two types of matching genres across the two data sources:\n",
    "1. Exact matches - IMDb and TMDb have the same exact name for a type of movie.\n",
    "2. Synonymous genres - IMDb list “Sci-Fi”, TMDb lists “Science Fiction”, and clearly these mean the same.\n",
    "Therefore, the first step is to convert all genres names to one common language.\n",
    "\n",
    "Then, we came up with the following options:\n",
    "1. Unioning lists of genres together for each movie. For example, if IMDb lists only Comedy, and TMDb lists only Family, our result would be Comedy and Family.\n",
    "2. Intersectioning lists of genres together for each movie. In other words, for a given movie, only include genres that are included on both websites.\n",
    "3. Considering genre counts across each data source, with the idea being if both sources list a given genre, that should be of stronger predictive value than only one source listing a genre, but we can still keep genres not listed from both sources.\n",
    "4. Using genres from just one data source, and completely ignoring the genre data from the other data source.\n",
    "\n",
    "After thinking this through, we believe that #2 is the best option here. The two data sources appear to have fairly consistent genre classifications in the first place, and by taking the intersection of genres across them, we can be more confident in our predictions. In the small number of cases where there is no common genre across the two data sources, we will include the genres from both databases (alternative: get rid of the movie in our dataset).\n",
    "\n",
    "To be clear, what we will be predicting is whether a given genre is listed for a movie on both IMDb and TMDb. We are therefore predicting a boolean for each genre for each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response variable: Choosing Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue of response variable posed a number of unique challenges. This was not a binary classification problem - we needed to predict movie genre across many categories as the EDA showed. Further complicating this challenge, the two databases we used had different genre labels and both allowed multiple genres to be assigned to each film. We settled on a multi-class approach as the various evaluation metrics would be easier to interpret and compare between various machine learning models.\n",
    "\n",
    "To assign a specific class to each movie, we first took the intersection of the TMDb and IMDb genres for each movie, based on the idea that if both databases assigned a specific genre or genres to a movie, then this genre was a more accurate representation. For those movies with multiple genres in the intersection, we initially decided to use joint genre categories (i.e. “Romance” and “Comedy” would be “Comedy - Romance”). However, the machine learning models were not able to distinguish these very well as compared to single-label genres, so instead we selected the genre that occurred more frequently in the rest of the data.\n",
    "\n",
    "This approach still left deeply imbalanced classes, with some genres (Drama, Comedy, Romance, Action, etc.) having 1,000+ records and others (War, Westerns, etc.) having <100 records. We therefore consolidated genres based on the team’s prior beliefs of similar themes, for instance combining Science-Fiction, Adventure, Action, War, Westerns, and Fantasy under “Action”. \n",
    "\n",
    "While there are still large imbalances between the genres, we believe there is enough of a sample size to train a model to distinguish between them and any further balancing could be handled using the in-built model functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbalanced Data: Some Genres Are Overrepresented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue to address is imbalanced genre frequencies. For example, from our exploratory data analysis, we saw that Drama is about 5 times as popular as Sci-Fi. We would not want our classifier to be heavily biased towards Drama, so we considered the following adjustments:\n",
    "1. Randomly undersampling the majority classes to have the same sample size as the minority class.\n",
    "2. Randomly oversampling the minority classes to have the same sample size as the majority class. This uses duplication of minority class examples, which seems less ideal than having distinct examples throughout the dataset. We believe that method 1 is more favorable than this method. We already will be dealing with a relatively large dataset, so oversampling minority classes (by duplication) in order to produce an even larger dataset seems unnecessary. Instead, we will have a training dataset where all samples are unique.\n",
    "3. Building cost-sensitive classifiers that take into account the imbalanced nature of the data when making predictions. We can adjust the prediction thresholds upwards or downwards from 0.5. While this method makes sense statistically, it will require adjustment of every model we build. Given the scope, timeline, and collaborative nature of the project, it may slow things down significantly.\n",
    "4. Guided undersampling of the majority classes: remove majority samples that are very similar to one another. A problem with this is that we don’t have an appropriate method of measuring distance between movies.\n",
    "5. Synthetic oversampling of the minority classes: create artificial examples of the minority class. Why do this when we have a large dataset available?\n",
    "\n",
    "For some of the reasons explained, we choose method 1: under sampling the majority classes.  \n",
    "\n",
    "In cases where we are not able to run the model on the entire dataset, we plan to undersample larger categories first to build a balanced dataset, and then sample randomly from the resulting dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from TMDb:\n",
    "* **Poster** - There may be visual information within each poster that is associated with each genre\n",
    "* **Part of Collection**- TMDb provides a `belongs_to_collection` feature which not all movies have. Based on some EDA it looks like the proportion of genres significantly depending on wheter or not movies are part of a collection. For example, dramas are 15 percentage points less of the total proportion of movies in a collection as compared to non-collection movies, while action and adventure movies are 5% more. This aligns with intuition since we would expect action and adventure movies to more likely to be parts of collections compared to dramas due to superhero movies and other action trilogies (Star Wars, Lord of the Rings, etc.).\n",
    "* **Budget** - As our EDA from the previous milestone illustrated, budget varies significantly depending on genre.\n",
    "* **Overview** - We plan to use both the length of the overview as well as some text analysis (tf - idf) since genres likely have different words describing the plot of the movie (i.e. love for romance, guns for action).\n",
    "* **Popularity** - Some genres could be more popular than other genres (ie. action vs. documentaries).\n",
    "* **# of Production Companies** - The number of production companies could vary by genre in a way similar to budget (more production companies needed for higher cost movies).\n",
    "* **# of Production Countries** - Some genres (action, sci-fi) may require more landscapes and environments than others (comedy, romance) do.\n",
    "* **Release Date** - We will look at both year and month. Genres may wax and wane in poularity over time (in which case year is useful), while some periods of the year are known for specific types of movies (action movies are released in the summer).\n",
    "* **Revenue** - Similar to popularity (the more consumers the more revenue)\n",
    "* **Runtime** - Some genres of movies may be longer than others (long action movies vs. short romantic comedies)\n",
    "* **# of Spoken Languages** - Similar to Production Countries\n",
    "* **Tagline** - Similar to Overview\n",
    "* **Length of Title** - Some genres may have longer titles than other genres\n",
    "* **Vote Average** - Assuming this is a proxy for quality, some genres may have on-average higher quality movies than other genres\n",
    "* **Vote Count** - Similar to Popularity / Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from IMDb:\n",
    "* **Various Department Sizes (animation, art, camera and electrical, casting, costume, editorial, makeup, music, special effects, visual)** - All of these not only serve a similar purpose to Budget in IMDb but also seem like they could vary in other ways un-related to buget (sci-fi movies may have higher special effects budgets)\n",
    "* **Cast Size** - Similar to Department Sizes\n",
    "* **# of Distributors** - Similar to Revnue in TMDb data (more distributors could signify more demand)\n",
    "* **MPAA** - The rating and rating description will be used. Ratings likely differ between genres (\"PG\" for Family vs. \"R\" for Horror), and the actual text of the rating (violence, language, etc.) could give a clue to the type of movie as well.\n",
    "* **Plot /Plot Outline** - Similar to Overview from TMDb\n",
    "* **Rating** - Similar to Vote Average in TMDb\n",
    "* **Votes** - Similar to Vote Count in TMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the features described in the previous milestone, we attemped to create features from the text fields (`overview`,`tagline`,`plot`,`plot_outline`, and `mpaa_rating`) by clustering them based on key words.\n",
    "\n",
    "To do this, we first transformed each text field into a tf-idf matrix which gets word counts for each record and then scales down the word counts based on the occurence of the word in the entire training set. As part of the tf-idf transformation we:\n",
    "* \"stemmed\" the words (\"fishing\", \"fished\", \"fisher\" would all be reduced to \"fish\")\n",
    "* filtered out common english terms (\"a\", \"the\", etc.)\n",
    "* used n-grams of lengths 1,2,and 3 (\"John\",\"John likes\", \"John likes movies\", etc.)\n",
    "* limited the max dataset frequency of a term to 10% (essentially the size of a genre as there are 11 genres) and the minimum datasest frequency to 5 (based on previous models we'd seen)\n",
    "\n",
    "With the transformation complete we applied k-means clustering to the tf-idf matrix, tuning the number of clusters based on whether the cluster key-words made intuitive sense with the genres in that cluster. For instance, `overview_cluster_3` had key words of \"murderer\", \"town\", \"killer\", \"investigator\", and \"detective\", and the top genres in this cluster were Thriller, Horror, and Drama-Thriller. For `overview_cluster_4`, which had key words of \"Love\", \"falls\", \"woman\",\"man\", and \"story\", the top genres were \"Drama-Romance\", \"Drama\", and \"Comedy-Romance\".\n",
    "\n",
    "All the code and clusters can be viewed here: https://github.com/All-Star-Vipers/CS109B-Final-Project/blob/master/Milestone%203/Text%20Analysis.ipynb\n",
    "\n",
    "Credit for some of the code and general approach goes to: http://brandonrose.org/clustering"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
